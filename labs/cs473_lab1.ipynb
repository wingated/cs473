{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPir_6bCCFnZ"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wingated/cs473/blob/main/labs/cs473_lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><p><b>After clicking the \"Open in Colab\" link, copy the notebook to your own Google Drive before getting started, or it will not save your work</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_slaQdUGCB0t"
   },
   "source": [
    "# BYU CS 473 Lab 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ct7fnkcnCL8O"
   },
   "source": [
    "## Introduction:\n",
    "Welcome to your first lab for CS 473, Advanced Machine Learning.\n",
    "\n",
    "In machine learning, models often predict *unnormalized log probabilities*. These must often be converted into regular probabilities.\n",
    "\n",
    "In this lab, you will explore the log-sum-exp function, which is described in the text (Sec. 2.5.4).  You will code up several variants of the function, and compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUat5xRAcdrC"
   },
   "source": [
    "---\n",
    "## Setup: The Iris Dataset\n",
    "We'll begin by downloading the Iris dataset. The iris dataset is a simple, but very famous, dataset introduced to the world by RA Fisher (the “father” of modern statistics”) in 1939. The dataset has five columns:\n",
    "* sepal length (cm)\n",
    "* sepal width (cm)\n",
    "* petal length (cm)\n",
    "* petal width (cm)\n",
    "* class\n",
    "\n",
    "In order to get logits to play with, we'll first train a multinomial logistic regression model (Sec. 2.5.3).  This model naturally outputs logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1m2KIHShNdC"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "ds = datasets.load_dataset( \"scikit-learn/iris\" )\n",
    "\n",
    "df = pd.DataFrame( ds['train'] )\n",
    "\n",
    "X = np.array( df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']] )\n",
    "Y = np.array( LabelEncoder().fit_transform( df['Species'] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5hV6PS0CwT8"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression().fit(X,Y)\n",
    "\n",
    "W = model.coef_\n",
    "b = model.intercept_\n",
    "\n",
    "b = np.reshape( b, (3,1))\n",
    "\n",
    "logits = np.dot( W, X.T ) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1: convert logits to probabilities\n",
    "\n",
    "Since our model outputs logits, they must be converted. To do this, we'll use the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3sgOg5oAvCv"
   },
   "outputs": [],
   "source": [
    "def softmax( logits ):\n",
    "    # logits is a numpy matrix of d x N\n",
    "    # where\n",
    "    #   d is the number of classes\n",
    "    #   N is the number of data points\n",
    "    # use equation 2.99 (see also Eq. 2.94)\n",
    "\n",
    "    # your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test cases\n",
    "probs = softmax( logits )\n",
    "probs[:,0]\n",
    "# array([9.81803910e-01, 1.81960759e-02, 1.43430317e-08])\n",
    "probs[:,120]\n",
    "# array([5.49519371e-06, 2.38812718e-02, 9.76113233e-01])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2: convert logits to probabilities\n",
    "\n",
    "Now, code up the logsumexp function.  What test cases should you use for this function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp( logits ):\n",
    "    # logits is a numpy matrix of d x N\n",
    "    # where\n",
    "    #   d is the number of classes\n",
    "    #   N is the number of data points\n",
    "    # use equation 2.100\n",
    "    \n",
    "    # your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cases\n",
    "probs = logsumexp( logits )\n",
    "probs[:,0]\n",
    "# what should be printed???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3: explore underflow / overlow\n",
    "\n",
    "First, code up a function that compares two distributions. This can be anything you want; you may consider things like the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_probs( probs1, probs2 ):\n",
    "    # your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs1 = softmax( logits )\n",
    "probs2 = logsumexp( logits )\n",
    "compare_probs( probs1, probs2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, see what happens if you add (or subtract) a constant from logits. How big must the constant be before things start going haywire?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "probs1 = softmax( logits + C )\n",
    "# etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert the logits to 16-bit precision, and re-run your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = logits.astype( np.float16 )\n",
    "\n",
    "# your code here\n",
    "probs1 = softmax( logits + C )\n",
    "# etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 4: cleanly compute log probabilities\n",
    "\n",
    "Sometimes, we want to compute log probabilities (which are different from logits), but we want to do so \"cleanly\", ie, while avoiding overflow / underflow. First, mathematically figure out what the log of the softmax is (ie, take the log of eq. 2.99), and then combine it with insights from coding up the logsumexp function. Hint: at the end of the day, you will simply shift each column by a per-column constant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_logsumexp( logits ):\n",
    "    # logits is a numpy matrix of d x N\n",
    "    # where\n",
    "    #   d is the number of classes\n",
    "    #   N is the number of data points\n",
    "\n",
    "    # your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
